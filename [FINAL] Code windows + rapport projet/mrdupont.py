from pyMarmoteMDP import * 
from time import *

critere = "min"
epsilon = 0.00001
maxIter = 500
dim_SS = 8
dim_SA = 10
actionSpace =marmoteInterval(0,8)
stateSpace = marmoteInterval(0,6)

trans=sparseMatrixVector(dim_SA)
reward = sparseMatrix(dim_SS,dim_SA)
P0 = sparseMatrix(dim_SS)
P0.addToEntry(0,0,1.0)
P0.addToEntry(1,0,1.0)
P0.addToEntry(2,2,1.0)
P0.addToEntry(3,3,1.0)
P0.addToEntry(4,4,1.0)
P0.addToEntry(5,5,1.0)
P0.addToEntry(6,6,1.0)
trans[0] = P0
P1 =sparseMatrix(dim_SS)
P1.addToEntry(0,1,0.1)
P1.addToEntry(0,2,0.9)
P1.addToEntry(1,1,1.0)
P1.addToEntry(2,2,1.0)
P1.addToEntry(3,3,1.0)
P1.addToEntry(4,4,1.0)
P1.addToEntry(5,5,1.0)
P1.addToEntry(6,6,1.0)
trans[1] = P1
P2 =sparseMatrix(dim_SS)
P2.addToEntry(0,3,0.2)
P2.addToEntry(0,4,0.7)
P2.addToEntry(0,5,0.1)
P2.addToEntry(1,1,1.0)
P2.addToEntry(2,2,1.0)
P2.addToEntry(3,3,1.0)
P2.addToEntry(4,4,1.0)
P2.addToEntry(5,5,1.0)
P2.addToEntry(6,6,1.0)
trans[2] = P2
P3 =sparseMatrix(dim_SS)
P3.addToEntry(0,6,1.0)
P3.addToEntry(1,1,1.0)
P3.addToEntry(2,2,1.0)
P3.addToEntry(3,3,1.0)
P3.addToEntry(4,4,1.0)
P3.addToEntry(5,5,1.0)
P3.addToEntry(6,6,1.0)
trans[3] = P3
P4 =sparseMatrix(dim_SS)
P4.addToEntry(0,0,1.0)
P4.addToEntry(1,1,0.1)
P4.addToEntry(1,2,0.9)
P4.addToEntry(3,3,1.0)
P4.addToEntry(4,4,1.0)
P4.addToEntry(5,5,1.0)
P4.addToEntry(6,6,1.0)
trans[4] = P4
P5 =sparseMatrix(dim_SS)
P5.addToEntry(0,0,1.0)
P5.addToEntry(1,1,1.0)
P5.addToEntry(2,6,1.0)
P5.addToEntry(3,3,1.0)
P5.addToEntry(4,4,1.0)
P5.addToEntry(5,5,1.0)
P5.addToEntry(6,6,1.0)

trans[5] = P5
P6 =sparseMatrix(dim_SS)
P6.addToEntry(0,0,1.0)
P6.addToEntry(1,1,1.0)
P6.addToEntry(2,2,1.0)
P6.addToEntry(3,6,1.0)
P6.addToEntry(4,4,1.0)
P6.addToEntry(5,5,1.0)
P6.addToEntry(6,6,1.0)
trans[6] = P6
P7 =sparseMatrix(dim_SS)
P7.addToEntry(0,0,1.0)
P7.addToEntry(1,1,1.0)
P7.addToEntry(2,2,1.0)
P7.addToEntry(3,3,1.0)
P7.addToEntry(4,6,1.0)
P7.addToEntry(5,5,1.0)
P7.addToEntry(6,6,1.0)
trans[7] = P7
P8 =sparseMatrix(dim_SS)
P8.addToEntry(0,0,1.0)
P8.addToEntry(1,1,1.0)
P8.addToEntry(2,2,1.0)
P8.addToEntry(3,3,1.0)
P8.addToEntry(4,4,1.0)
P8.addToEntry(5,6,1.0)
P8.addToEntry(6,6,1.0)
trans[8] = P8

Reward  = sparseMatrix(dim_SS, dim_SA)
Reward.addToEntry(0,0,2)
Reward.addToEntry(1,0,2)
Reward.addToEntry(2,0,2)
Reward.addToEntry(3,0,2)
Reward.addToEntry(4,0,2)
Reward.addToEntry(5,0,2)
Reward.addToEntry(0,1,2)
Reward.addToEntry(1,1,2)
Reward.addToEntry(2,1,2)
Reward.addToEntry(3,1,2)
Reward.addToEntry(4,1,2)
Reward.addToEntry(5,1,2)
Reward.addToEntry(0,2,1)
Reward.addToEntry(1,2,1)
Reward.addToEntry(2,2,1)
Reward.addToEntry(3,2,1)
Reward.addToEntry(4,2,1)
Reward.addToEntry(5,2,1)
Reward.addToEntry(0,3,45)
Reward.addToEntry(1,3,45)
Reward.addToEntry(2,3,45)
Reward.addToEntry(3,3,45)
Reward.addToEntry(4,3,45)
Reward.addToEntry(5,3,45)
Reward.addToEntry(0,4,9)
Reward.addToEntry(1,4,9)
Reward.addToEntry(2,4,9)
Reward.addToEntry(3,4,9)
Reward.addToEntry(4,4,9)
Reward.addToEntry(5,4,9)
Reward.addToEntry(0,5,35)
Reward.addToEntry(1,5,35)
Reward.addToEntry(2,5,35)
Reward.addToEntry(3,5,35)
Reward.addToEntry(4,5,35)
Reward.addToEntry(5,5,35)
Reward.addToEntry(0,6,20)
Reward.addToEntry(1,6,20)
Reward.addToEntry(2,6,20)
Reward.addToEntry(3,6,20)
Reward.addToEntry(4,6,20)
Reward.addToEntry(5,6,20)
Reward.addToEntry(0,7,30)
Reward.addToEntry(1,7,30)
Reward.addToEntry(2,7,30)
Reward.addToEntry(3,7,30)
Reward.addToEntry(4,7,30)
Reward.addToEntry(5,7,30)
Reward.addToEntry(0,8,10)
Reward.addToEntry(1,8,10)
Reward.addToEntry(2,8,10)
Reward.addToEntry(3,8,10)
Reward.addToEntry(4,8,10)
Reward.addToEntry(5,8,10)

mdp1 =totalRewardMDP(critere, stateSpace, actionSpace, trans, Reward)
mdp1.writeMDP()

t= time()
optimum = mdp1.valueIteration(epsilon, maxIter)
print(time() -t)

t = time()
optimum = mdp1.policyIterationModified(epsilon, maxIter,epsilon,maxIter)
print(time()-t)







